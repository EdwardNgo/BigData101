{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loved-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/viethoang/petproject/20202/BigData101/data/2015flight.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "polish-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cooperative-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder.master(\"local\").appName(\"dfex\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "embedded-slope",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.16.111:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dfex</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f97f021baf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-tribe",
   "metadata": {},
   "source": [
    "### 1. Read file to dataframe\n",
    "For faster access of data, use df.cache() to put the data in memory When to use caching: As suggested in this post, it is recommended to use caching in the following situations:\n",
    "\n",
    "* RDD re-use in iterative machine learning applications\n",
    "* RDD re-use in standalone Spark applications\n",
    "* When RDD computation is expensive, caching can help in reducing the cost of recovery in the case one executor fails\n",
    "##### df.cache() is lazy operation, it does not cache the data until you use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "connected-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.read.format('csv').load(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "harmful-forum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "constant-basket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aging-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use option to read header\n",
    "df = sc.read.format('csv').option('header', True).option('inferSchema', True).load(data_path)\n",
    "df.show(3)\n",
    "df.printSchema()  # now count is in IntegerType!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dutch-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache the dataframe\n",
    "df.cache() # cache it on memory if the table will be access frequently\n",
    "df.createOrReplaceTempView('dfTable')  # this is for running the sql code on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-trance",
   "metadata": {},
   "source": [
    "### 2. Load file with manual schema\n",
    "* Control data type \n",
    "* Avoid precision issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ambient-sacramento",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "# df have a schema attribute\n",
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hungry-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify a schema manually - e.g. what if count is long rather than integer\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "# field name, data type, nullable\n",
    "field_1 = StructField('DEST_COUNTRY_NAME', StringType(), True)\n",
    "field_2 = StructField('ORIGIN_COUNTRY_NAME', StringType(), False)\n",
    "field_3 = StructField('count', LongType(), False)\n",
    "\n",
    "manualSchema = StructType([field_1, field_2, field_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceramic-oklahoma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sc.read.format('csv').option('header', True).schema(manualSchema).load(data_path)\n",
    "df.show(3) \n",
    "\n",
    "# now count is long! \n",
    "# but nullable is true\n",
    "# this is because CSV format doesn't provide any tools which allow you to specify data constraints \n",
    "# so by definition reader cannot assume that input is not null and your data indeed contains nulls.\n",
    "df.printSchema()\n",
    "#csv không cho phép ràng buộc dữ liệu "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-fusion",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Column, Row, and Create DataFrame from Scratch \n",
    "col can be used in expression for select() method.\n",
    "\n",
    "row is data records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "pharmaceutical-level",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|              NYC|                MIA|    2|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col\n",
    "# create a DataFrame from Scratch\n",
    "# field name, data type, nullable\n",
    "field_1 = StructField('DEST_COUNTRY_NAME', StringType(), True)\n",
    "field_2 = StructField('ORIGIN_COUNTRY_NAME', StringType(), True)\n",
    "field_3 = StructField('count', LongType(), False)\n",
    "\n",
    "manualSchema = StructType([field_1, field_2, field_3])\n",
    "\n",
    "newRow = Row('NYC', 'MIA', 2)\n",
    "\n",
    "newDF = sc.createDataFrame([newRow], manualSchema)\n",
    "newDF.show()\n",
    "#cơ bản thì giống pandas có thêm 2 cái class col,row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-superior",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Select & SelectExpr \n",
    "Flexible expression on columns data.\n",
    "\n",
    "* use expr() in methods such as select() and withColumn()\n",
    "* use selectExpr() instead of select(expr(..), expr(..), ..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "conceptual-beginning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "advised-shoulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|ORIGIN_COUNTRY_NAME|\n",
      "+-------------------+\n",
      "|            Romania|\n",
      "|            Croatia|\n",
      "|            Ireland|\n",
      "|      United States|\n",
      "+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "former-requirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|  destination|    departure|\n",
      "+-------------+-------------+\n",
      "|United States|      Romania|\n",
      "|United States|      Croatia|\n",
      "|United States|      Ireland|\n",
      "|        Egypt|United States|\n",
      "+-------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# use AS to rename the column name\n",
    "# the returned new DataFrame will have a new column name\n",
    "newdf = df.select(expr(\"DEST_COUNTRY_NAME as destination\"), \n",
    "          expr('ORIGIN_COUNTRY_NAME as departure'))\n",
    "newdf.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "swiss-variable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------+\n",
      "|  destination|    departure|invalid|\n",
      "+-------------+-------------+-------+\n",
      "|United States|      Romania|  false|\n",
      "|United States|      Croatia|  false|\n",
      "|United States|      Ireland|  false|\n",
      "|        Egypt|United States|  false|\n",
      "+-------------+-------------+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use * to select all column\n",
    "# expr can take some more operations between columns to create new column\n",
    "newdf.select('*', expr('destination=departure as invalid')).show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "coordinate-eleven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------+---+-----+\n",
      "|  destination|    departure|invalid|two| true|\n",
      "+-------------+-------------+-------+---+-----+\n",
      "|United States|      Romania|  false|  1|false|\n",
      "|United States|      Croatia|  false|  1|false|\n",
      "|United States|      Ireland|  false|  1|false|\n",
      "|        Egypt|United States|  false|  1|false|\n",
      "+-------------+-------------+-------+---+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.selectExpr('*', 'destination=departure as invalid','1 as two','false as true').show(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-nation",
   "metadata": {},
   "source": [
    "### 5. Column Manipulation \n",
    "For maniplating column\n",
    "\n",
    "* use df.withColumn() to add column, change column type.\n",
    " * this method takes two params: column name, expression\n",
    " * if column name does not exist, it will append a new column\n",
    " * if column name exist, it will replace the column with new expression result\n",
    "* use df.withColumnRenamed() to rename a column\n",
    " * column name should avoid reserved characters and keywords such as as. If needed, use `...` to skip.\n",
    " * use df.drop() to drop a column\n",
    " * Spark session is by default case insensitive.\n",
    "\n",
    "* use spark_session.sql('set spark.sql.caseSensitive=false') to change to case sensitive\n",
    "* Add an id column:\n",
    "\n",
    " * monotonically_increasing_id() return an id column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-insulin",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Row Manipulation \n",
    "Basic Manipluation including:\n",
    "\n",
    "* Filtering: filter rows with some expression with T/F output: df.filter() or df.where()\n",
    " * note that df.select(condition) will return a table with single column of true/false.\n",
    " * Get Unique: df.distinct()\n",
    " * Random Samples: df.sample(withReplacement=, fraction=, seed=)\n",
    " * Random Splits: df.randomSplit(fractions=, seed=)\n",
    " * Concat and Append: df.union(\n",
    " * Sort: df.orderBy(expr or col or col_name)\n",
    " * Limit: df.limit(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdataenv",
   "language": "python",
   "name": "bigdataenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
